\documentclass[10pt, a4paper, english]{article}
%typesetting
\usepackage[margin = 1in]{geometry} % margins
\usepackage[T1]{fontenc} % font encoding
\usepackage{babel} %enables typesetting for multiple languages
\usepackage{parskip} %new lines
\usepackage{graphicx} 
\usepackage{float}
\floatplacement{figure}{H} %when printing tables, include  table.position="H"
\usepackage{bm}
\usepackage{amsmath}

\usepackage[dvipsnames]{xcolor} % more colors

\usepackage[colorlinks]{hyperref}


 %clickable table of contents from hyperref
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\usepackage[colorinlistoftodos]{todonotes}


\title{Machine Learning 2ST129 26605 HT2023
 Assignment }
\author{Anonymous Student}
\date{\today}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage

 <<echo=FALSE, results=FALSE>>=
knitr::opts_chunk$set(
  comment = '', fig.width = 6, fig.height = 6, echo=FALSE, warning=FALSE, message=FALSE,
  eval=TRUE
)
@


\section*{General Information}
\begin{itemize}
\item Time used for reading: 
\item Time used for basic assignment:
\item Time used for extra assignment 
\item Good with lab 
\item Things to improve in the lab: 
\end{itemize}

<<>>=
#Libraries
 library(tidyverse)
 library(xtable)
 library(keras)
 library(tensorflow)
@

\section{Task 1}
\subsection{1.1}
First we instantiate and  visualize the data.
<<>>=
 mnist <- dataset_mnist() 
x_train <- mnist$train$x/255
x_test<- mnist$test$x/255


#' function to iterate through the different idx and plot the images
plot_images <- function(image_array, y, idx, pixel_dim = 28, ncol = 3, pred=FALSE) {
  par(mfrow =c(4,2))
  
  for (i in idx) {
    im <- image_array[i,,]
    im <- t(apply(im, 2, rev))
    if (isFALSE(pred)){
      main <- paste(y[i])
    } else{
main <- paste("Actual: ", paste0(y[i],","), "Predicted:", pred[i])
    }
  image(1:pixel_dim, 1:pixel_dim, im, col = gray((0:255)/255),
            xlab = "", ylab = "", xaxt = 'n', yaxt = 'n',
        main = main)
  }
  
  par(mfrow = c(1, 1))  
}

@

<<imgaes, fig.cap="Visualization of digits" >>=
plot_images(image_array = x_train, y = mnist$train$y, idx = c(1:8))
@


\subsection{1.2}

The dataset contains of 60k 28x28 grayscale images of the 10 digits, as well as a test set with 10k images. 
<<>>=
str(mnist$train)
str(mnist$test)
@



\subsection{1.3}
Now we want to implement a simple-feed-forward neural network with one hidden layer with 16 units and by using the sigmoid activation function. The steps done here to modify the data or model is as proposed by the guide Getting Started with Keras. 


<<eval=TRUE, echo = TRUE >>=

#Since the reponse variablbe y is an vector with integer values with 10 classes, we need to one-hot encode them into binary class matrices
#
y_train <- to_categorical(mnist$train$y, num_classes = 10)  
y_test <- to_categorical(mnist$test$y, num_classes = 10)




# Model architecture
model <- keras_model_sequential(input_shape = c(28,28)) %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = 'sigmoid') %>%  # Hidden layer with 16 units and sigmoid activation
  layer_dense(units = 10, activation = 'softmax')  # Output layer with softmax activation


@

<<>>=
print(model)
@
The model has  12730 parameters in total. The input layer has 12560 and the output layer has 170.
Next we compile the model and fit it to compute the validation accuracy. For computational reasons we can set the \textttt{batch\_size} to 128 to make it run faster,  while also setting \texttt{validation_split()} to 0.2 so that it for every epoch uses 20 percent of the data as validation.  
<<eval=FALSE, echo = FALSE >>=
# Compile the model
#here we choose the "adam optimizer as well as categorical_crossentropy
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# Train the model
#here we choose batch_size = 128 and validaiton split = 0.2
#the number of samples used in each iteration of gradient descent  which is more computationally efficient
# Validation is the fraction of the training data that is reserved for validation. 

history <- model %>% fit(
  x_train, y_train,
  epochs = 30,batch_size = 128, 
  validation_split = 0.2
)

  #Evaluate the model
accuracy <- model %>% evaluate(x_test, y_test)

cat("The loss is", accuracy[1], "\nThe accuracy is", accuracy[2])
@
Based on the results, the accuracy is about 94 percent and the validation accuracy is .

\subsection{1.4}
Now we make some adjustments to the model by implementing the following:
\begin{itemize}
\item Increase the number of hidden units to 128.
\item Change the activation function to reLU. 
\item Change the optimizer to RMSprop. 
\item Add a second layer with 128 hidden units. 
\item Add dropout with 0.2 dropout probability for both of  hidden layers. 
\item Add batch normalization for both of hidden layers.
\end{itemize}

<<>>=

# Model architecture
model_new <- keras_model_sequential(input_shape = c(28,28)) %>%
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>%  # Hidden layer with 128 units and relu activation. More learning capacity but also more overfitting
  layer_dropout(0.2) %>%  #dropping out random neurons with p=0.2
  layer_batch_normalization() %>% 
  layer_dense(units=128, activation="relu") %>% #second layer
  layer_dropout(0.2) %>% 
  layer_batch_normalization() %>% 
  layer_dense(units = 10, activation = 'softmax')  # Output layer with softmax activation

  #batch normalization: batch normalization, reparametrizes the model
#in a way that introduces both additive and multiplicative noise on the hidden units at training time
#
model_new %>% compile(
  optimizer = 'RMSprop',# RMSPROP instead of adam
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

history <- model_new %>% fit(
  x_train, y_train,
  epochs = 30,batch_size = 128, 
  validation_split = 0.2
)
#Evaluate the model
accuracy_new <- model_new %>% evaluate(x_test, y_test)

cat("The loss is", accuracy_new[1], "\nThe accuracy is", accuracy_new[2])

@
Looking at the results, then the accuracy has increased to around 0.98.

\subsection{1.5} If i were to use early stopping, then for each epoch we keep track of the parameter values as well as the accuracy.  Then in order to obtain a model with better validation error, I would return the parameter setting for the point in which the validation set accuracy was the highest.  If had do to implement it from scratch, then to the best of my knowledge I would have to write my own callback. But otherwise i probably can just use the defined method \texttt{Earlystopping} and something along the lines of:
<<eval=FALSE>>=
callback <- list(callback_early_stopping(patience = 5))
history <- model %>% fit(
  #rest of arguments here
  #....
  callbacks = callback
)
@


\subsection{1.6}
For this task we want to find the best possible model. I tried to vary different arguments and parameters. In the end i just decided to stick with the following changes:
\begin{itemize}
\item Added 3 new hidden layers, each with relu activation. 
\item Added L\_2 regularizer for all hidden layers in order to..
\item Specified that the images are in greyscale by adding 1 in input shape. But i do not know if it actually will affect performance.
\item Created a function \texttt{Scheduler()} to decrease the learning rate exponentially after 10 epochs
\item Implemented Early stopping.
\item increased batch size to 256
\item Increased validation split to 0.3

<<eval=TRUE>>=

model_best <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(28, 28, 1)) %>% 
  layer_dense(units = 256, activation = 'relu', kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(0.3) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 256, activation = 'relu',kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(0.3) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 256, activation = 'relu',kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dropout(0.3) %>%
  layer_batch_normalization() %>%
  layer_dropout(0.3) %>% 
   layer_dense(units=128, activation="relu",kernel_regularizer = regularizer_l2(0.001)) %>% 
  layer_batch_normalization() %>% 
  layer_dropout(0.3) %>% 
   layer_dense(units=64, activation="relu", kernel_regularizer = regularizer_l2(0.001)) %>% 
  layer_batch_normalization() %>% 
  layer_dropout(0.3) %>% 
  layer_dense(units = 10, activation = 'softmax')

model_best %>% compile(
  optimizer = 'RMSprop',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)


Scheduler <- function(epoch, lr) {
  if (epoch < 10) {
    return(lr)
  } else {
    return(lr * exp(-0.1))
  }
}

callback_list = list(callback_early_stopping(patience = 10),
                     callback_learning_rate_scheduler(Scheduler))
 
history <- model_best %>% fit(
  x_train, y_train,
  epochs = 50,batch_size = 256, 
  validation_split = 0.2,
  callbacks =callback_list,
  verbose = 1)



  #validation_data = list(x_test, y_test)
 accuracy_best <- model_best %>% evaluate(x_test, y_test)

@

\subsection{1.7}
Now we identify two digits that the network has classified incorrectly. 
<<>>=
predictions <- model_best %>% predict(x_test)
predicted_labels <- max.col(predictions) - 1  # Assuming one-hot encoding (subtract 1 to convert to 0-based indexing)
true_labels <- max.col(y_test) - 1  # Assuming one-hot encoding
true_labels
predicted_labels
predictions
max.col(y_test)
predictions
incorrect_indices <- which(predicted_labels != true_labels)
incorrect_images <- x_test[incorrect_indices, ,]

plot_images(image_array = x_test, , y=mnist$test$y,pred = predicted_labels, idx = incorrect_indices[1:8])

plot_images(image_array = x_train, y = y_train, idx = incorrect_indices[1:8])

y_test[incorrect_indices]
library(ggplot2)

# Assuming each image is a matrix with dimensions 28x28
n_rows <- 5
n_cols <- 5

par(mfrow=c(n_rows, n_cols), mar=c(0, 0, 1, 0))

for (i in 1:8) {
  img <- matrix(incorrect_images[i,, ], nrow = 28, ncol = 28)
  image(1:28, 1:28, img, axes = FALSE, col = gray.colors(256))
}

#they look really scuffed

@

Figure \ref{fig:} shows the some of the wrongly classified images with the predicted values above each image. Looking at them, Then based on the images i would say its not really that surprising seeing as some of them look really scuffed and even I cannot really tell what some of them are supposed to be.
\subsection{1.8}
Now we compute the accuracy on the hold-out test set.
<<>>=

 accuracy_best <- model_best %>% evaluate(x_test, y_test)
@
 It seems that it is 98.4 percent, which is still in line with the validation accuracy of the model, but slightly less than the normal accuracy. 
 
 It is perhaps not too surprising since the model was tuned as much as possible to get the best accuracy for the validation sets. Still it is rather close, but perhaps it was just a bit overfitted in comparison to the hold out set, since I changed teh parameters and tried different options in order to get as good accuracy as possible for the validation sets only. 
 
 \subsection{Task 2}
 \subsection{2.1}
<<>>=
W <- matrix(1, nrow = 2, ncol = 2)
c <- matrix(c(0, -1) , ncol = 2, nrow = 4, byrow = TRUE) 
X <- matrix(c(0, 0, 1, 1, 0, 1, 0, 1), ncol = 2) 
w <- matrix(c(1, -2) , ncol = 1) 
b <- 0
mini_net <- function(X, W, c, w, b){
  
}
y_test
@
 
\end{document}


%layers typ lika med funktioner



@


@
